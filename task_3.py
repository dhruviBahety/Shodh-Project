# -*- coding: utf-8 -*-
"""Task-3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ck_x8zdLJT7oYR8wBi3CEWoqixEo7y3B
"""

# Task 3: Offline Reinforcement Learning for Loan Approval
# Using Conservative Q-Learning (CQL) - Modern Offline RL Algorithm


import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
print("="*60)
print("TASK 3: OFFLINE REINFORCEMENT LEARNING")
print("="*60)

# Load preprocessed data
data = np.load('processed_data.npz', allow_pickle=True)
X_train, X_test = data['X_train'], data['X_test']
y_train, y_test = data['y_train'], data['y_test']

# Load original data for reward calculation
df_train = np.load('processed_data.npz', allow_pickle=True)
print(f"\nData loaded:")
print(f"  Train: {X_train.shape[0]:,} samples")
print(f"  Test:  {X_test.shape[0]:,} samples\n")

print("RL Environment Definition:")
print("-" * 60)
print("  State (s):  Applicant features (vector)")
print("  Action (a): {0: Deny, 1: Approve}")
print("  Reward (r):")
print("    - Deny:    r = 0 (no risk, no gain)")
print("    - Approve & Paid:    r = +loan_amnt × int_rate (profit)")
print("    - Approve & Default: r = -loan_amnt (loss)")

def calculate_reward(loan_amount, interest_rate, defaulted, action):

    if action == 0:  # Deny
        return 0
    else:  # Approve
        if defaulted == 1:
            return -loan_amount  # Lose principal
        else:
            return loan_amount * interest_rate  # Earn interest


np.random.seed(42)
loan_amnt_train = np.random.lognormal(9.5, 0.5, len(X_train))
int_rate_train = np.random.uniform(0.05, 0.25, len(X_train))
loan_amnt_test = np.random.lognormal(9.5, 0.5, len(X_test))
int_rate_test = np.random.uniform(0.05, 0.25, len(X_test))

print("\n✓ Reward function defined")

def prepare_offline_data(X, y, loan_amounts, interest_rates):
    states = X
    actions = np.ones(len(X))  # Historical policy: approve all
    rewards = np.array([
        calculate_reward(loan_amounts[i], interest_rates[i], y[i], actions[i])
        for i in range(len(X))
    ])

    next_states = states
    dones = np.ones(len(X))

    return states, actions, rewards, next_states, dones

states_train, actions_train, rewards_train, next_states_train, dones_train = \
    prepare_offline_data(X_train, y_train, loan_amnt_train, int_rate_train)

states_test, actions_test, rewards_test, next_states_test, dones_test = \
    prepare_offline_data(X_test, y_test, loan_amnt_test, int_rate_test)

print(f"\nOffline RL Dataset:")
print(f"  States shape: {states_train.shape}")
print(f"  Actions: {np.unique(actions_train)} (historical: all approved)")
print(f"  Avg reward: ${rewards_train.mean():,.2f}")
print(f"  Reward std:  ${rewards_train.std():,.2f}")
print(f"  Total profit (train): ${rewards_train.sum():,.2f}")

class CQLAgent(nn.Module):
    """
    Conservative Q-Learning Agent

    CQL is a state-of-the-art offline RL algorithm that:
    - Learns Q-values conservatively to avoid overestimation
    - Penalizes Q-values for actions not seen in the dataset
    - Works well with static datasets (no environment interaction)

    Paper: Kumar et al., "Conservative Q-Learning for Offline RL" (NeurIPS 2020)
    """
    def __init__(self, state_dim, n_actions=2):
        super().__init__()
        self.q_network = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),

            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),

            nn.Linear(128, 64),
            nn.ReLU(),

            nn.Linear(64, n_actions)  # Q-values for both actions
        )

    def forward(self, state):
        return self.q_network(state)

    def get_action(self, state, epsilon=0.0):
        """Get action (0 or 1) based on Q-values"""
        if np.random.random() < epsilon:
            return np.random.randint(0, 2)
        with torch.no_grad():
            q_values = self.forward(state)
            return q_values.argmax().item()

# Initialize networks
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
state_dim = X_train.shape[1]

agent = CQLAgent(state_dim).to(device)
target_agent = CQLAgent(state_dim).to(device)
target_agent.load_state_dict(agent.state_dict())

print(f"\n✓ CQL Agent initialized")
print(f"  Architecture: {state_dim} → 256 → 128 → 64 → 2")
print(f"  Device: {device}")

def train_cql(agent, target_agent, states, actions, rewards, next_states, dones,
              epochs=50, batch_size=512, lr=0.0003, gamma=0.0, alpha=1.0):
    """
    Train Conservative Q-Learning agent

    Args:
        alpha: CQL penalty coefficient (higher = more conservative)
        gamma: Discount factor (0 for single-step, 0.99 for long-term)
    """
    optimizer = optim.Adam(agent.parameters(), lr=lr)

    # Convert to tensors
    states_t = torch.FloatTensor(states).to(device)
    actions_t = torch.LongTensor(actions.astype(int)).to(device)
    rewards_t = torch.FloatTensor(rewards).to(device)
    next_states_t = torch.FloatTensor(next_states).to(device)
    dones_t = torch.FloatTensor(dones).to(device)

    history = {'q_loss': [], 'cql_loss': [], 'total_loss': [], 'avg_q': []}
    n_samples = len(states)

    print("\nTraining CQL Agent...")
    print("-" * 60)

    for epoch in range(epochs):
        epoch_q_loss = 0
        epoch_cql_loss = 0
        epoch_q_vals = []

        indices = np.random.permutation(n_samples)

        for start in range(0, n_samples, batch_size):
            batch_idx = indices[start:start + batch_size]

            batch_states = states_t[batch_idx]
            batch_actions = actions_t[batch_idx]
            batch_rewards = rewards_t[batch_idx]
            batch_next_states = next_states_t[batch_idx]
            batch_dones = dones_t[batch_idx]
            current_q = agent(batch_states)
            current_q_actions = current_q.gather(1, batch_actions.unsqueeze(1)).squeeze()

            with torch.no_grad():
                next_q = target_agent(batch_next_states).max(1)[0]
                target_q = batch_rewards + gamma * next_q * (1 - batch_dones)

            bellman_loss = nn.functional.mse_loss(current_q_actions, target_q)

            dataset_q = current_q.gather(1, batch_actions.unsqueeze(1))
            all_q = current_q.logsumexp(dim=1, keepdim=True)
            cql_penalty = (all_q - dataset_q).mean()

            # Total loss
            loss = bellman_loss + alpha * cql_penalty

            # Optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_q_loss += bellman_loss.item()
            epoch_cql_loss += cql_penalty.item()
            epoch_q_vals.append(current_q.mean().item())

        # Update target network
        if (epoch + 1) % 5 == 0:
            target_agent.load_state_dict(agent.state_dict())

        # Record metrics
        n_batches = n_samples // batch_size
        history['q_loss'].append(epoch_q_loss / n_batches)
        history['cql_loss'].append(epoch_cql_loss / n_batches)
        history['total_loss'].append((epoch_q_loss + alpha * epoch_cql_loss) / n_batches)
        history['avg_q'].append(np.mean(epoch_q_vals))

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:2d}/{epochs} | "
                  f"Q-Loss: {history['q_loss'][-1]:.4f} | "
                  f"CQL-Loss: {history['cql_loss'][-1]:.4f} | "
                  f"Avg Q: {history['avg_q'][-1]:.2f}")

    return history

# Train the agent
history = train_cql(agent, target_agent, states_train, actions_train,
                    rewards_train, next_states_train, dones_train,
                    epochs=50, alpha=1.0)

print("\n✓ Training complete!")

print("\n" + "="*60)
print("EVALUATION")
print("="*60)

agent.eval()

# Get RL policy decisions on test set
rl_decisions = []
rl_q_values = []

with torch.no_grad():
    for state in torch.FloatTensor(states_test).to(device):
        q_vals = agent(state)
        action = q_vals.argmax().item()
        rl_decisions.append(action)
        rl_q_values.append(q_vals.cpu().numpy())

rl_decisions = np.array(rl_decisions)
rl_q_values = np.array(rl_q_values)

# Calculate financial metrics
rl_rewards = []
for i in range(len(states_test)):
    reward = calculate_reward(loan_amnt_test[i], int_rate_test[i],
                              y_test[i], rl_decisions[i])
    rl_rewards.append(reward)

rl_rewards = np.array(rl_rewards)

# Baseline: Historical policy (approve all)
baseline_rewards = []
for i in range(len(states_test)):
    reward = calculate_reward(loan_amnt_test[i], int_rate_test[i],
                              y_test[i], action=1)
    baseline_rewards.append(reward)

baseline_rewards = np.array(baseline_rewards)

# Results
approval_rate = rl_decisions.mean() * 100
total_profit = rl_rewards.sum()
baseline_profit = baseline_rewards.sum()
improvement = (total_profit - baseline_profit) / abs(baseline_profit) * 100

print(f"\n1. Financial Performance:")
print(f"   RL Policy:")
print(f"     - Total Profit:   ${total_profit:,.2f}")
print(f"     - Approval Rate:  {approval_rate:.1f}%")
print(f"     - Avg per Loan:   ${rl_rewards.mean():,.2f}")
print(f"\n   Baseline (Approve All):")
print(f"     - Total Profit:   ${baseline_profit:,.2f}")
print(f"     - Approval Rate:  100.0%")
print(f"     - Avg per Loan:   ${baseline_rewards.mean():,.2f}")
print(f"\n   Improvement: {improvement:+.1f}%")

# Default rate of approved loans
approved_idx = rl_decisions == 1
if approved_idx.sum() > 0:
    rl_default_rate = y_test[approved_idx].mean()
    baseline_default_rate = y_test.mean()
    print(f"\n2. Risk Management:")
    print(f"   RL Policy Default Rate:       {rl_default_rate:.2%}")
    print(f"   Baseline Default Rate:        {baseline_default_rate:.2%}")
    print(f"   Risk Reduction:               {(baseline_default_rate - rl_default_rate):.2%}")

# Classification metrics (treat as binary classifier)
print(f"\n3. Decision Quality:")
print(f"   (Treating 'approve good loan' as positive class)")
print("-" * 60)
# Optimal action: approve if not default (y=0), deny if default (y=1)
optimal_actions = 1 - y_test
print(classification_report(optimal_actions, rl_decisions,
                           target_names=['Should Deny', 'Should Approve']))

# 7. COMPARISON WITH SUPERVISED MODEL (TASK 2)

print("\n" + "="*60)
print("COMPARISON: SUPERVISED vs OFFLINE RL")
print("="*60)

# Load supervised predictions
task2_data = np.load('task2_predictions.npz')
sl_pred_proba = task2_data['y_pred_proba']
sl_decisions = (sl_pred_proba < 0.5).astype(int)

# Calculate supervised model rewards
sl_rewards = []
for i in range(len(states_test)):
    reward = calculate_reward(loan_amnt_test[i], int_rate_test[i],
                              y_test[i], sl_decisions[i])
    sl_rewards.append(reward)

sl_rewards = np.array(sl_rewards)
sl_profit = sl_rewards.sum()
sl_approval_rate = sl_decisions.mean() * 100

print(f"\nFinancial Performance:")
print(f"  {'Strategy':<20} {'Total Profit':<20} {'Approval Rate':<15} {'Avg/Loan'}")
print("-" * 70)
print(f"  {'Baseline (All)':<20} ${baseline_profit:>15,.2f}   {100:>6.1f}%      ${baseline_rewards.mean():>10,.2f}")
print(f"  {'Supervised (DL)':<20} ${sl_profit:>15,.2f}   {sl_approval_rate:>6.1f}%      ${sl_rewards.mean():>10,.2f}")
print(f"  {'Offline RL (CQL)':<20} ${total_profit:>15,.2f}   {approval_rate:>6.1f}%      ${rl_rewards.mean():>10,.2f}")

print(f"\nKey Insight:")
if total_profit > sl_profit:
    diff = (total_profit - sl_profit) / abs(sl_profit) * 100
    print(f"  ✓ RL outperforms Supervised by {diff:.1f}%")
    print(f"    Reason: Direct optimization of financial reward")
else:
    diff = (sl_profit - total_profit) / abs(total_profit) * 100
    print(f"  ✓ Supervised outperforms RL by {diff:.1f}%")
    print(f"    May need hyperparameter tuning (alpha, gamma)")


fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# Training curves
axes[0, 0].plot(history['q_loss'], label='Q-Loss', linewidth=2)
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].set_title('Q-Learning Loss')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

axes[0, 1].plot(history['cql_loss'], label='CQL Penalty', linewidth=2, color='red')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('Penalty')
axes[0, 1].set_title('Conservative Q-Learning Penalty')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

axes[0, 2].plot(history['avg_q'], label='Avg Q-Value', linewidth=2, color='purple')
axes[0, 2].set_xlabel('Epoch')
axes[0, 2].set_ylabel('Q-Value')
axes[0, 2].set_title('Average Q-Value')
axes[0, 2].legend()
axes[0, 2].grid(alpha=0.3)

# Q-value distribution
axes[1, 0].hist(rl_q_values[:, 0], bins=50, alpha=0.6, label='Deny', color='red')
axes[1, 0].hist(rl_q_values[:, 1], bins=50, alpha=0.6, label='Approve', color='green')
axes[1, 0].set_xlabel('Q-Value')
axes[1, 0].set_ylabel('Frequency')
axes[1, 0].set_title('Q-Value Distribution by Action')
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)

# Action distribution
action_counts = np.bincount(rl_decisions)
axes[1, 1].bar(['Deny', 'Approve'], action_counts, color=['red', 'green'], alpha=0.7)
axes[1, 1].set_ylabel('Count')
axes[1, 1].set_title('RL Policy Decisions')
for i, v in enumerate(action_counts):
    axes[1, 1].text(i, v + 100, f'{v:,}\n({v/len(rl_decisions)*100:.1f}%)',
                    ha='center', fontweight='bold')
axes[1, 1].grid(axis='y', alpha=0.3)

# Profit comparison
strategies = ['Baseline\n(Approve All)', 'Supervised\n(Deep Learning)', 'Offline RL\n(CQL)']
profits = [baseline_profit, sl_profit, total_profit]
colors = ['gray', 'blue', 'purple']
axes[1, 2].bar(strategies, profits, color=colors, alpha=0.7)
axes[1, 2].set_ylabel('Total Profit ($)')
axes[1, 2].set_title('Profit Comparison')
axes[1, 2].ticklabel_format(style='plain', axis='y')
for i, v in enumerate(profits):
    axes[1, 2].text(i, v + max(profits)*0.01, f'${v/1e6:.2f}M',
                    ha='center', fontweight='bold')
axes[1, 2].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('task3_results.png', dpi=300)
print("\n✓ Visualizations saved to 'task3_results.png'")

# Save results
torch.save(agent.state_dict(), 'cql_agent.pth')
np.savez('task3_results.npz',
         rl_decisions=rl_decisions,
         rl_q_values=rl_q_values,
         rl_rewards=rl_rewards,
         total_profit=total_profit,
         approval_rate=approval_rate)

print("✓ Model saved to 'cql_agent.pth'")
print("✓ Results saved to 'task3_results.npz'")

print("\n" + "="*60)
print("TASK 3 COMPLETE")
print("="*60)
print(f"\nKey Achievements:")
print(f"  ✓ Implemented Conservative Q-Learning (CQL)")
print(f"  ✓ Total Profit: ${total_profit:,.2f}")
print(f"  ✓ Approval Rate: {approval_rate:.1f}%")
print(f"  ✓ Improvement over baseline: {improvement:+.1f}%")
print(f"\nReady for final analysis and comparison!")